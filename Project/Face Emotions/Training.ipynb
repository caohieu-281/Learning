{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dự đoán cảm xúc với mạng Neural Network (Training)](https://mechasolution.vn/blog)\n",
    "## **Editor:** Lương Công Tâm - [MechasolutionVN](https://mechasolution.vn)\n",
    "<u>**Credit:**</u> [Emotion Recognition - Omar Ayman](https://github.com/omar178/Emotion-recognition)\n",
    "\n",
    "------------------------\n",
    "Dataset: [fer2013](https://www.kaggle.com/c/3364/download-all)\n",
    "<br>\n",
    "<u>**Ghi chú:**</u> Dataset *fer2013* không Public, vì thế mình sẽ không viết script để tự tải về. Bạn phải tự tải về và copy file **CSV** vào thư mục **fer2013**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị\n",
    "### Import các thư viện\n",
    "Để chạy được chương trình huấn luyện mạng Neural Network này, ngoài các thư viện trong bài viết, bạn cần phải cài thêm các thư viện Pandas và Scikit Learn. Để cài đặt, chạy dòng lệnh sau trong **Command Promt/Terminal**\n",
    "```bash\n",
    "pip install pandas sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các thư viện để xây dựng Neural Network\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Các thư viện để train\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khai báo các hàm, các tham số và cấu trúc Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'fer2013/fer2013.csv'\n",
    "base_path = 'models/' \n",
    "image_size=(48,48)\n",
    "batch_size = 32\n",
    "num_epochs = 10000\n",
    "input_shape = (48, 48, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "\n",
    "# Hàm load dataset từ file CSV\n",
    "def load_fer2013():\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        pixels = data['pixels'].tolist()\n",
    "        width, height = image_size\n",
    "        \n",
    "        faces = []\n",
    "        for pixel_sequence in pixels:\n",
    "            face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "            face = np.array(face).reshape(width, height)\n",
    "            face = cv2.resize(face.astype('uint8'),image_size)\n",
    "            faces.append(face.astype('float32'))\n",
    "        faces = np.array(faces)\n",
    "        faces = np.expand_dims(faces, -1)\n",
    "        emotions = pd.get_dummies(data['emotion']).values\n",
    "        return faces, emotions\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    "\n",
    "# Xây dựng mạng Neural Network. KEYWORD: XCEPTION \n",
    "def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
    "    regularization = l2(l2_regularization)\n",
    "\n",
    "    # base\n",
    "    img_input = Input(input_shape)\n",
    "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "                                            use_bias=False)(img_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "                                            use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # module 1\n",
    "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 2\n",
    "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 3\n",
    "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    # module 4\n",
    "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                      padding='same', use_bias=False)(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                        kernel_regularizer=regularization,\n",
    "                        use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = Conv2D(num_classes, (3, 3),\n",
    "            #kernel_regularizer=regularization,\n",
    "            padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Activation('softmax',name='predictions')(x)\n",
    "\n",
    "    model = Model(img_input, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Khởi tạo Model và Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo Data Generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "# Khởi tạo model\n",
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Nơi lưu file log và models\n",
    "log_file_path = base_path + 'emotion_training.log'\n",
    "model_names = base_path + 'emotion_model.accuracy_{val_acc:.2f}.hdf5'\n",
    "\n",
    "# Khởi tạo Callback cho quá trình train\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nạp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, emotions = load_fer2013()\n",
    "faces = preprocess_input(faces)\n",
    "num_samples, num_classes = emotions.shape\n",
    "\n",
    "# Chia dataset ra làm 2 bộ: train, validation\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(data_generator.flow(xtrain, ytrain, batch_size),\n",
    "                    steps_per_epoch=len(xtrain) / batch_size,\n",
    "                    epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                    validation_data=(xtest,ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
