{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6bde510",
   "metadata": {},
   "source": [
    "Chào mọi người, trong quá trình viết về [AdaBoost](https://viblo.asia/p/adaboost-buoc-di-dau-cua-boosting-gAm5yrGwKdb) của, mình có tìm được 2 bài về Ensemble Learning [Ensemble learning và các biến thể (P1)\n",
    "](https://viblo.asia/p/ensemble-learning-va-cac-bien-the-p1-WAyK80AkKxX) và [Gradient Boosting - Tất tần tật về thuật toán mạnh mẽ nhất trong Machine Learning](https://viblo.asia/p/gradient-boosting-tat-tan-tat-ve-thuat-toan-manh-me-nhat-trong-machine-learning-YWOZrN7vZQ0) của các anh. Hai bài đã giải thích rất rõ để mọi người hiểu thế nào là mô hình học yếu, cách để kết hợp chúng lại với nhau thông qua Bagging và Boosting, nhưng chưa có bài viết nào làm rõ chi tiết về Stacking. Mình thấy không viết tiếp một bài về Stacking thì bất công quá!(sad) Vậy nên trong bài viết này mình giới thiệu đến mọi người thuật toán tiếp theo trong series '''Ensemble learning và các biến thể ''' - Stacking.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9012a",
   "metadata": {},
   "source": [
    "# **I. Tổng quan về Stacking**\n",
    "\n",
    "Stacking hay têm đầy đủ là Stacked Generalization là một thuật toán học máy thuộc Ensemble Learning. Tương tự như Bagging và Boosting, Stacking cũng kết hợp các dự đoán từ nhiều mô hình học máy trên cùng một tập dữ liệu.\n",
    "\n",
    "Như chúng ta đã biết, việc kết hợp thật nhiều mô hình giống nhau giúp cải thiện kết quả dự đoán. Nhưng với nhiều mô hình học máy, mỗi mô hình có những điểm lợi và hại, và có những cách riêng để đưa ra dự đoán, vậy làm sao để có thể chọn ra mô hình nào tốt nhất, hay làm sao để có thể kết hợp các đặc tính tốt của các mô hình lại với nhau? Câu trả lời là Stacking. Stacking mang đến góc nhìn mới về cách kết hợp các mô hình học máy lại với nhau.\n",
    "\n",
    "![Stacking Image](https://images.viblo.asia/7175acfc-25a5-4019-9c7a-a7f03f11710c.png)\n",
    "\n",
    "- Khác với Bagging, Stacking sử dung nhiều mô hình học máy khác nhau (ko chỉ là Decision Tree) và học trên cùng một tập dữ liệu (thay vì các [samples](https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60) của tập huấn luyện )\n",
    "- Khác với Boosting, sử dụng chuỗi mô hình để tự sửa chữa những sai lầm của mô hình trước, Stacking sử dụng duy nhất một mô hình để đưa ra dự đoán tốt nhất từ các dự đoán của các mô hình khác\n",
    "\n",
    "Mô hình Stacking cơ bản thường được phân thành 2 cấp là level-0 modes và level-1 model:\n",
    "- Level-0 Models (Base-Models): Mô hình cơ sở học trực tiếp từ bộ dữ liệu và đưa ra dự đoán cho mô hình level-1\n",
    "- Level-1 Model (Meta-Model): Mô hình học từ các dự đoán của mô hình cơ sở (level-0) \n",
    "\n",
    "Có nghĩa Meta-model được huấn luyện dựa trên đầu ra dự đoán của các base-modes, các outputs này kết hợp với nhãn của bài toán tạo thành cặp dữ liệu đầu vào - đầu ra trong quá trình huấn luyện Meta-model. Có thể thấy Meta-model không học trực tiếp từ tập dữ liệu huấn luyện, tuy nhiên, việc dùng dữ liệu bàn đầu thêm vào outputs của base-models vẫn hoàn toàn hợp lý, sẽ cấp thêm cho meta-model nhiều thông tin hơn về dữ liệu.\n",
    "\n",
    "Các base-models có những cách học khác nhau trên bộ dữ liệu, cho nên outputs hay errors của các base-models là không tương quan (uncorrelated) hay có độ tương quan thấp (low correlation).\n",
    "\n",
    "Đầu ra của base-models có thể là giá trị thực (cho bài toán Hồi quy) hoặc là các xác suất của nhãn trong bài toán phân loại.\n",
    "\n",
    "Hmmm, có nghĩa là: Giả sử sắp tới bạn phải ôn thi, mà bạn lại có vài đứa bạn, đứa nào cũng giỏi nhưng ai cũng có môn lại giỏi hơn những người khác . Mà bạn lại lười phải học từ đầu, nên bạn tĩnh tâm đợi bạn bè học xong, rồi nhờ bạn bè dạy lại (lmao), bạn sẽ học được nhiều nhất từ mọi người (vì ai cũng học rồi, đều học được những điều hay, kiến thức đã được chắt lọc), (maybe) bạn giỏi toàn diện và điểm sẽ cao hơn (sure kèo) =))\n",
    "\n",
    "Ví dụ trên có vẻ hơi ảo nhưng đấy cũng là cách về cơ bản Stacking hoạt động, bây giờ mình sẽ đi vào chi tiết hơn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a1f3e",
   "metadata": {},
   "source": [
    "# **II. Stacking Algorithm**\n",
    "\n",
    "Pseudo code của Stacking được đưa ra lần đầu (mình không chắc lắm) bởi David H trong \"Stacked generalization\" năm 1992 (wow) và cũng là mô hình cơ bản nhất của Stacking.\n",
    "\n",
    "![](https://images.viblo.asia/e75d2b9c-b627-4c48-9fa4-bdff9ef3be7e.png)\n",
    "\n",
    "Được chia thành 3 bước chính:\n",
    "- Bước thứ 1: Sử dụng các base-models để học trên toàn bộ dữ liệu và đưa ra kết quả dự đoán ban đầu\n",
    "- Bước thứ 2: Xây dựng bộ dữ liệu mới dựa trên outputs của các base-models\n",
    "- Bước thứ 3: Huấn luyện Meta-model với bộ dữ liệu mới và đưa ra kết quả cuối cùng\n",
    "\n",
    "![](https://images.viblo.asia/b83e6f12-31e1-41c5-9414-ef1ef6307dd5.png)\n",
    "\n",
    "Ý tưởng mô hình là rất tốt nhưng nó lại tồn tại 1 vấn đề khá nghiêm trọng. Đó là rất dễ bị overfitting. Tại sao?\n",
    "\n",
    "Việc học từ kết quả của các mô hình trước dẫn đến việc, nếu giả sử $C_1$ là Depth Decision Tree, và kết quả của $C_1$ bị overfitting trên bộ dữ liệu, (và chỉ một) $C_1$ bị overffiting, thì Meta-model sẽ bị phụ thuộc khá nhiều vào kết quả của $C_1$ dẫn đến việc toàn bộ cấu trúc bị overfitting theo. Và đó cũng là lý do khiến mô hình Stacking cơ bản này dễ bị overfitting. \n",
    "\n",
    "Để giải quyết vấn đề này, có rất nhiều phương pháp được đưa ra, về cơ bản, mặc dù có thể không hoàn toàn tránh được overfitting nhưng ít nhất cũng làm giảm đc hiện tượng đó, các phương pháp đó là\n",
    "\n",
    "## **2.1 Stacking với Cross-Validation**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
