{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WoX0B0-KVzDO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DU2xtkpsVzDZ",
    "outputId": "31ebb45c-15d3-4214-d4ef-42b94caa7a69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>FamilySize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  FamilySize\n",
       "0         0       3    0  22.0           2\n",
       "1         1       1    1  38.0           2\n",
       "2         1       3    1  26.0           1\n",
       "3         1       1    1  35.0           2\n",
       "4         0       3    0  35.0           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/titanic.csv')\n",
    "\n",
    "# Data manipulation\n",
    "titanic.fillna(titanic['Age'].mean(), inplace=True)\n",
    "titanic.replace({'Sex':{'male':0, 'female':1}}, inplace=True)\n",
    "titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
    "titanic.drop(columns=['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKgSlEt3VzDb",
    "outputId": "059b02ac-cf5e-4e51-c368-01291eb1adc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (801, 4) (801, 1)\n",
      "Test set: (90, 4) (90, 1)\n"
     ]
    }
   ],
   "source": [
    "X = titanic[['Pclass', 'Sex', 'Age', 'FamilySize']].values\n",
    "y = titanic[['Survived']].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Training set:', x_train.shape, y_train.shape)\n",
    "print('Test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bCBBqEhXVzDd"
   },
   "outputs": [],
   "source": [
    "def voting_quantity(lis):\n",
    "    return max(set(lis), key=list(lis).count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4gv5GOyZVzDd"
   },
   "outputs": [],
   "source": [
    "def training(model,train,y,test,n_fold):\n",
    "    folds=StratifiedKFold(n_splits=n_fold, random_state=42, shuffle= True)\n",
    "    train_pred= [0 for i in range(train.shape[0])]\n",
    "    test_pred = []\n",
    "    \n",
    "    for i_train, i_val in folds.split(train,y):\n",
    "        x_train, y_train, y_train, y_val = [], [], [], []\n",
    "    \n",
    "        x_train, x_val = train[i_train], train[i_val]\n",
    "        y_train, y_val = y[i_train], y[i_val]\n",
    "\n",
    "        model.fit(X=x_train, y=y_train)\n",
    "        \n",
    "        # Add to train_pred\n",
    "        pre = model.predict(x_val)\n",
    "        for i,local in enumerate(i_val):\n",
    "            train_pred[local] = pre[i]\n",
    "            \n",
    "        # Add to test_pred\n",
    "        test_pred.append(model.predict(test))\n",
    "    \n",
    "    # Voting result\n",
    "    lis = [[] for i in range(test.shape[0])]\n",
    "    for i in range(test.shape[0]):\n",
    "        for j in range(len(test_pred)):\n",
    "            lis[i].append(test_pred[j][i])\n",
    "            \n",
    "    rel = []\n",
    "    for i in range(test.shape[0]):\n",
    "        rel.append(voting_quantity(lis[i]))\n",
    "    return rel, train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifiers = [('Decision Tree', DecisionTreeClassifier()),\n",
    "                    ('KNN', KNeighborsClassifier()),\n",
    "                    ('Naive Bayes', GaussianNB()),\n",
    "                    ('Logistic Regression', LogisticRegression()),\n",
    "                    ('Random Forest', RandomForestClassifier()),\n",
    "                    ('AdaBoost Classifier', AdaBoostClassifier()),\n",
    "                    ('Gradient Boosting Classifier', GradientBoostingClassifier()),\n",
    "                    ('Extra Trees Classifier', ExtraTreesClassifier())]\n",
    "# ('SVM', SVC(kernel='linear', C=1e3)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking(base_classifiers, n_fold, x_train, x_test,y_train):\n",
    "    test_pred ,train_pred = [0 for i in range(len(base_classifiers))], [0 for i in range(len(base_classifiers))]\n",
    "\n",
    "    for i, model in enumerate(base_classifiers):\n",
    "        test_pred[i], train_pred[i] = training(model= model[i],n_fold=n_fold, train=x_train,test=x_test,y=y_train)\n",
    "        print(\"Training \", model[1], \"done!!!\")\n",
    "        print(\"<+++======================+++>\")\n",
    "        \n",
    "    x_test_pred = [[] for i in range(np.array(test_pred).shape[1])]\n",
    "    for i in range(np.array(test_pred).shape[1]):\n",
    "        for j in range(len(test_pred)):\n",
    "            x_test_pred[i].append(test_pred[j][i])\n",
    "            \n",
    "    x_train_pred = [[] for i in range(np.array(train_pred).shape[1])]\n",
    "    for i in range(np.array(train_pred).shape[1]):\n",
    "        for j in range(len(train_pred)):\n",
    "            x_train_pred[i].append(train_pred[j][i])\n",
    "            \n",
    "    return x_test_pred, x_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "392wD8fDVzDf",
    "outputId": "ac7aeafd-bbe4-4039-9b7e-30f25cf79488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  DecisionTreeClassifier() done!!!\n",
      "<+++======================+++>\n",
      "Training  KNeighborsClassifier() done!!!\n",
      "<+++======================+++>\n",
      "Training  GaussianNB() done!!!\n",
      "<+++======================+++>\n",
      "Training  LogisticRegression() done!!!\n",
      "<+++======================+++>\n",
      "Training  RandomForestClassifier() done!!!\n",
      "<+++======================+++>\n",
      "Training  AdaBoostClassifier() done!!!\n",
      "<+++======================+++>\n",
      "Training  GradientBoostingClassifier() done!!!\n",
      "<+++======================+++>\n"
     ]
    }
   ],
   "source": [
    "x_test_pred, x_train_pred = Stacking(base_classifiers,10,x_train, x_test,y_train)\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x_train_pred, y_train)\n",
    "rel = model.predict(x_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy score: \", accuracy_score(y_test, rel))\n",
    "print(\"F1-score: \", f1_score(y_test, rel))\n",
    "print(\"Recall-score: \", recall_score(y_test,rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n",
    "\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None        # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111111111111111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"Random Forest classifier. Uses a collection of classification trees that\n",
    "    trains on random subsets of the data using a random subsets of the features.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    max_features: int\n",
    "        The maximum number of features that the classification trees are allowed to\n",
    "        use.\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_gain: float\n",
    "        The minimum impurity required to split the tree further. \n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_features=None, min_samples_split=2,\n",
    "                 min_gain=0, max_depth=float(\"inf\")):\n",
    "        self.n_estimators = n_estimators    # Number of trees\n",
    "        self.max_features = max_features    # Maxmimum number of features per tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gain = min_gain            # Minimum information gain req. to continue\n",
    "        self.max_depth = max_depth          # Maximum depth for tree\n",
    "        self.progressbar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "\n",
    "        # Initialize decision trees\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            self.trees.append(\n",
    "                ClassificationTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_impurity=min_gain,\n",
    "                    max_depth=self.max_depth))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # If max_features have not been defined => select it as\n",
    "        # sqrt(n_features)\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(math.sqrt(n_features))\n",
    "\n",
    "        # Choose one random subset of the data for each tree\n",
    "        subsets = get_random_subsets(X, y, self.n_estimators)\n",
    "\n",
    "        for i in self.progressbar(range(self.n_estimators)):\n",
    "            X_subset, y_subset = subsets[i]\n",
    "            # Feature bagging (select random subsets of the features)\n",
    "            idx = np.random.choice(range(n_features), size=self.max_features, replace=True)\n",
    "            # Save the indices of the features for prediction\n",
    "            self.trees[i].feature_indices = idx\n",
    "            # Choose the features corresponding to the indices\n",
    "            X_subset = X_subset[:, idx]\n",
    "            # Fit the tree to the data\n",
    "            self.trees[i].fit(X_subset, y_subset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_preds = np.empty((X.shape[0], len(self.trees)))\n",
    "        # Let each tree make a prediction on the data\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            # Indices of the features that the tree has trained on\n",
    "            idx = tree.feature_indices\n",
    "            # Make a prediction based on those features\n",
    "            prediction = tree.predict(X[:, idx])\n",
    "            y_preds[:, i] = prediction\n",
    "            \n",
    "        y_pred = []\n",
    "        # For each sample\n",
    "        for sample_predictions in y_preds:\n",
    "            # Select the most common class prediction\n",
    "            y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444444444444444"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = RandomForestClassifier()\n",
    "cl.fit(x_train, y_train)\n",
    "pr = cl.predict(x_test)\n",
    "accuracy_score(pr, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Stacking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
