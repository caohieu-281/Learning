{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chào mọi người, hôm \n",
    "\n",
    "\n",
    "Với:\n",
    "- Tập dữ liệu $\\{(x_1, y_1), (x_2, y_2),..., (x_n, y_n)\\}$, với $y_i \\in \\{-1, 1\\}, i \\in \\{1, 2, ..., n\\}$\n",
    "- Trọng số các điểm dữ liệu tại weak leaner thứ t:  $w_1^t, w_2^t,..., w_n^t, i \\in \\{1, 2, ..., n\\}$\n",
    "- Weak learners $h: x\\rightarrow\\{-1, 1\\}$\n",
    "- Error function: $E(f(x), y, i) = e^{-y_i f(x_i)}$, trong đó $f(x_i) = \\alpha h(x_i)$\n",
    "- Output: $H_t(x)$, $H_t(x)$ còn được gọi là strong leaner tại thời điểm t\n",
    "\n",
    "Các bước triển khai cho thuật toán được mô tả như sau:\n",
    "\n",
    "B1: Khởi tạo weights cho từng input: $w_{i}^{1} = \\frac{1}{n}, i \\in \\{1, 2, ..., n\\} \\rightarrow \\sum_{i}{w_i^{1}} = 1$\n",
    "\n",
    "B2: Với mỗi vòng lặp $t \\in \\{1, 2,..., T\\}$:\n",
    "- Tìm weak learners $h_{t} (x)$ để tối thiểu hóa tổng error của các điểm bị phân loại sai,  $E = \\sum_{y_i \\# h_t(x_i)}{w_i^t}$\n",
    "- Tỉ lệ lỗi của weak leaners: $\\varepsilon_{t} = \\frac{\\sum_{y_i \\# h_t(x_i)}{w_i^t}}{\\sum_{i=1}^{n}{w_{i}^t}} = \\sum_{y_i \\# h_t(x_i)}{w_i^t}$, vì $\\sum_{i=1}^{n}{w_{i}^t} = 1$ :D\n",
    "- Gán trọng số cho weak learners giá trị $\\alpha_{t} = \\frac{1}{2}\\ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t})$\n",
    "- Cập nhật strong learner: $H_{t}(x) = H_{t-1}(x) + \\alpha_{t}h_t(x)$\n",
    "- Cập nhật lại weights:\n",
    "    - $w_{i}^{t+1} = w_{i}^{t} e^{- y_i \\alpha_t h_t(x_i)}, i \\in \\{1, 2, ..., n\\}$\n",
    "    - Chuẩn hóa lại weights sao cho $\\sum_{i}{w_i^{t+1}} = 1$\n",
    " \n",
    "B3: Ouput chính là dấu của biểu thức tổng các weak learners nhân với trọng số của chúng, hay\n",
    "$$H(x) = sign(\\sum_{t=1}^T{\\alpha_t h_t(x)})$$\n",
    "\n",
    "## Tại sao $\\alpha_t = \\frac{1}{2}ln(\\frac{1 - \\varepsilon_{t}}{\\varepsilon_{t}})$ ?\n",
    "\n",
    "Việc đặt lại trọng số cho các weak learners là vô cùng quan trọng, ở đây, thuật toán đã chỉ ra rằng khi $\\alpha_{t} = \\frac{1}{2}\\ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t})$, chúng ta có thể tối thiểu hóa hàm loss $L_{exp} = e^{-yh(x)}$ trong quá trình training.\n",
    "\n",
    "Như thuật toán ở trên, sau vòng lặp thứ $(t-1)$, strong leaner là tổ hợp tuyến tính của các weak leaners dưới dạng biểu thức:\n",
    "$$H_{t-1}(x_i) = \\alpha_1h_1(x_i)  + \\alpha_2h_2(x_i) + ... + \\alpha_{t-1}h_{t-1}(x_i)$$\n",
    "Class của điểm $x_i$ chính là dấu của biểu thức $H_{t}(x_i)$. Tại vòng lặp $t$, để tăng độ chính xác cho $H(x)$, ta cộng thêm weak learners $h_t$ nhân với trọng số $\\alpha_t$ của nó:\n",
    "$$H_{t}(x_i) = H_{t-1}(x_i) + \\alpha_{t}h_t(x_i)$$\n",
    "Gọi $E$ là tổng *exponential loss* của $H_t$ trên từng điểm dữ liệu, ta có\n",
    "\n",
    "$$E = \\sum_{i=1}^{N}{e^{-y_i H_t(x_i)}}$$\n",
    "$$ = \\sum_{i=1}^{N}{e^{-y_i (H_{t-1}(x_i) + \\alpha_{t}h_t(x_i))}}$$\n",
    "$$ = \\sum_{i=1}^{N}{e^{-y_i H_{t-1}(x_i)}e^{-y_i \\alpha_t h_t(x_i)}}$$\n",
    "\n",
    "Đặt trọng số $w_i^1$ = $\\frac{1}{n}$ và $w_i^t = e^{-y_i H_{t-1}(x_i)}$ với t > 1, ta được:\n",
    "$$E = \\sum_{i=1}^{N}{w_i^te^{-y_i \\alpha_t h_t(x_i)}}$$\n",
    "\n",
    "Nhận thấy có 2 khả năng xảy ra với mỗi điểm trong tập dữ liệu:\n",
    "\n",
    "-  Phân loại đúng: $y_i = h_t(x_i)$\n",
    "-  Phân loại sai: $y_i$ # $h_t(x_i)$\n",
    "\n",
    "    \n",
    "Khi đó:\n",
    "$$E = \\sum_{y_i = h_t(x_i)}{w_i^te^{-\\alpha_t}} + \\sum_{y_i \\# h_t(x_i)}{w_i^te^{\\alpha_t}}$$\n",
    "\n",
    "Đến đây chỉ cần sử dụng kiến thức đạo hàm lớp 12 để tối thiểu hóa loss $E$ theo $\\alpha_t$, như sau:\n",
    "$$ \\frac{\\delta E}{\\delta \\alpha_t} = \\frac{\\delta(\\sum_{y_i = h_t(x_i)}{w_i^te^{-\\alpha_t}} + \\sum_{y_i \\# h_t(x_i)}{w_i^te^{\\alpha_t}})}{\\delta \\alpha_t}$$\n",
    "$$ = -\\sum_{y_i = h_t(x_i)}{w_i^te^{-\\alpha_t}} + \\sum_{y_i \\# h_t(x_i)}{w_i^te^{\\alpha_t}} = 0$$\n",
    "Vì $e^{-\\alpha_t}$ và $e^{\\alpha_t}$ không phụ thuộc vào i, nên:\n",
    "$$ e^{-\\alpha_t}\\sum_{y_i = h_t(x_i)}{w_i^t} = e^{\\alpha_t}\\sum_{y_i \\# h_t(x_i)}{w_i^t}$$\n",
    "Lấy Loga Nepe 2 vế,\n",
    "$$-\\alpha_t + ln(\\sum_{y_i = h_t(x_i)}{w_i^t}) = \\alpha_t + ln(\\sum_{y_i \\# h_t(x_i)}{w_i^t})$$\n",
    "Chuyển vế,\n",
    "$$-2\\alpha_t = ln(\\sum_{y_i \\# h_t(x_i)}{w_i^t}) - ln(\\sum_{y_i = h_t(x_i)}{w_i^t})$$\n",
    "Áp dụng $lna - lnb = ln\\frac{a}{b}$,\n",
    "$$-2\\alpha_t = ln(\\frac{\\sum_{y_i \\# h_t(x_i)}{w_i^t}}{ \\sum_{y_i = h_t(x_i)}{w_i^t}})$$\n",
    "$$\\alpha_t = \\frac{1}{2}ln(\\frac{\\sum_{y_i = h_t(x_i)}{w_i^t}}{ \\sum_{y_i \\# h_t(x_i)}{w_i^t}})$$\n",
    "Vì tỉ lệ lỗi của weak leaners $\\varepsilon_{t} = \\sum_{y_i \\# h_t(x_i)}{w_i^t} / \\sum_{i=1}^{n}{w_{i}^t}$ và $\\sum_{y_i = h_t(x_i)}{w_i^t} + \\sum_{y_i \\# h_t(x_i)}{w_i^t}\n",
    "= \\sum_{i=1}^{n}{w_{i}^t}$ nên,\n",
    "$$\\alpha_t = \\frac{1}{2}ln(\\frac{\\sum_{i=1}^{n}{w_{i}^t} - \\sum_{y_i \\# h_t(x_i)}{w_i^t}}{ \\sum_{y_i \\# h_t(x_i)}{w_i^t}})$$\n",
    "$$\\alpha_t = \\frac{1}{2}ln(\\frac{1}{\\varepsilon_{t}} - 1)$$\n",
    "$$\\alpha_t = \\frac{1}{2}ln(\\frac{1 - \\varepsilon_{t}}{\\varepsilon_{t}})$$\n",
    "\n",
    "Vậy ta đã có được điều phải chứng minh :D\n",
    "\n",
    "Lý thuyết vậy đủ rồi, cùng thử code nào :triumph:\n",
    "\n",
    "# II. Implement model\n",
    "\n",
    "## **2.1 Các thư viện cần dùng**\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "\n",
    "## **2.2 Các hàm hỗ trợ**\n",
    "\n",
    "Vì $y$ nhận 2 giá trị -1 và 1, nên mình cần có 1 hàm kiểm tra gíá trị của $y$\n",
    "```\n",
    "def check(y):\n",
    "    assert set(y) == {-1,1}\n",
    "    return y\n",
    "```\n",
    "Tiếp theo, để tiện theo dõi cách mà AdaBoost hoạt động, hàm $plot\\_adaboost$ tạo ra để biểu diễn các điểm dữ liệu và decision boundary của chúng\n",
    "```\n",
    "def plot_adaboost(X: np.ndarray,\n",
    "                  y: np.ndarray,\n",
    "                  clf=None,\n",
    "                  sample_weights: Optional[np.ndarray] = None,\n",
    "                  ax: Optional[mpl.axes.Axes] = None) -> None:\n",
    "\n",
    "    y = check(y) # Kì vọng nhãn bằng ±1\n",
    "\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n",
    "        fig.set_facecolor('white')\n",
    "\n",
    "    pad = 1\n",
    "    x_min, x_max = X[:, 0].min() - pad, X[:, 0].max() + pad\n",
    "    y_min, y_max = X[:, 1].min() - pad, X[:, 1].max() + pad\n",
    "\n",
    "    if sample_weights is not None:\n",
    "        sizes = np.array(sample_weights) * X.shape[0] * 100\n",
    "    else:\n",
    "        sizes = np.ones(shape=X.shape[0]) * 100\n",
    "\n",
    "    X_pos = X[y == 1]\n",
    "    sizes_pos = sizes[y == 1]\n",
    "    ax.scatter(*X_pos.T, s=sizes_pos, marker='+', color='red')\n",
    "\n",
    "    X_neg = X[y == -1]\n",
    "    sizes_neg = sizes[y == -1]\n",
    "    ax.scatter(*X_neg.T, s=sizes_neg, marker='.', c='blue')\n",
    "    \n",
    "    if clf:\n",
    "        plot_step = 0.01\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        if list(np.unique(Z)) == [1]:\n",
    "            fill_colors = ['r']\n",
    "        else:\n",
    "            fill_colors = ['b', 'r']\n",
    "\n",
    "        ax.contourf(xx, yy, Z, colors=fill_colors, alpha=0.2)\n",
    "\n",
    "    ax.set_xlim(x_min+0.5, x_max-0.5)\n",
    "    ax.set_ylim(y_min+0.5, y_max-0.5)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "```\n",
    "\n",
    "## **2.3 Tạo dữ liệu**\n",
    "\n",
    "Mình sử dụng bộ dữ liệu đã đơn giản hóa từ [sklearn documentation](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html)\n",
    "\n",
    "```\n",
    "def make_dataset(n: int = 100, random_seed: int = None):\n",
    "    \n",
    "    n_per_class = int(n/2)\n",
    "    \n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    X, y = make_gaussian_quantiles(n_samples=n, n_features=2, n_classes=2)\n",
    "    \n",
    "    return X, y*2-1\n",
    "\n",
    "X, y = make_dataset(n=30, random_seed=10)\n",
    "plot_adaboost(X, y)\n",
    "```\n",
    "\n",
    "![](https://images.viblo.asia/e5cfa24d-fb92-415a-a424-d2c3115e2d59.png)\n",
    "\n",
    "\n",
    "## **2.4 Xây dựng model**\n",
    "\n",
    "Bảng dưới đây giải thích các biến mình sử dụng trong code mà ý nghĩa của chúng:\n",
    "\n",
    "|Tên biến|Chiều|Kí hiệu trong giải thuật|Ý nghĩa|\n",
    "|---|---|---|---|\n",
    "|sample_weights|(T,n)|$w_i^t$|Trọng số của input|\n",
    "|stumps|(T,)|$h_t(x)$|Cây con (weak learners), Decision Tree|\n",
    "|stump_weights|(T,)|$\\alpha_t$|Trọng số weak learners|\n",
    "|errors|(T,)|$\\varepsilon_t$|Tỉ lệ lỗi của weak learners|\n",
    "|predict(X)|y.shape|$H_t(x)$|Ouput thuật toán|\n",
    "|current_sew|||Weights tại cây thứ t|\n",
    "|new_sew|||Cập nhật lại weight cho cây t+1|\n",
    "\n",
    "Cũng oke r đấy, bắt đầu thôi!\n",
    "\n",
    "**Khởi tạo các biến**\n",
    "```\n",
    "def init_model(iters, X):\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    sample_weights = np.zeros((iters, n))\n",
    "    stumps = np.zeros(iters, dtype= object)\n",
    "    stump_weights = np.zeros(iters)\n",
    "    errors = np.zeros(iters)\n",
    "    return stumps, stump_weights, sample_weights, errors\n",
    "```\n",
    "\n",
    "**Triển khai mô hình**\n",
    "Tóm tắt lại thuật toán: \n",
    "**Thử phân loại dữ liệu bằng  *sklearn.ensemble.AdaBoostClassifier***\n",
    "\n",
    "```\n",
    "clf = AdaBoostClassifier(n_estimators=15, algorithm='SAMME').fit(X, y)\n",
    "plot_adaboost(X, y, clf=clf)\n",
    "\n",
    "train_err = (clf.predict(X) != y).mean()\n",
    "print(f'Train error: {train_err:.1%}')\n",
    "```\n",
    "![](https://images.viblo.asia/f4c17870-b90f-4092-8fa8-0015b46a5588.png)\n",
    "\n",
    "Vì các class tách biệt khá rõ ràng nên *AdaBoostClassifier* dễ dàng fits toàn bộ dữ liệu chỉ với 15 cây con\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\sum_{y_ih_t(x_i) = 1}{w_i^te^{-\\alpha_t}} + \\sum_{y_ih_t(x_i) = -1}{w_i^te^{-\\alpha_t}} + \\sum_{y_ih_t(x_i) = -1}{w_i^te^{\\alpha_t}} - \\sum_{y_ih_t(x_i) = -1}{w_i^te^{-\\alpha_t}}$$\n",
    "$$ = \\sum_{1}^{N}{w_i^te^{-\\alpha_t}} + \\sum_{y_ih_t(x_i) = -1}{w_i^t(e^{\\alpha_t}} - e^{-\\alpha_t})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
